{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import json\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Define a dataset class specifically for handling text (captions)\n",
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self, json_file):\n",
    "        with open(json_file, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        caption = self.data[idx]['updated_caption']\n",
    "        return caption, idx\n",
    "\n",
    "# Function to compute text embeddings and update the JSON file\n",
    "def compute_text_embeddings_and_update_json(json_file, model, device):\n",
    "    dataset = CaptionDataset(json_file)\n",
    "    loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    embeddings = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for captions, idxs in loader:\n",
    "            captions = clip.tokenize(captions).to(device)\n",
    "            \n",
    "            # Get text embeddings\n",
    "            text_embeddings = model.encode_text(captions)\n",
    "            \n",
    "            # Normalize embeddings\n",
    "            normalized_text_embeddings = text_embeddings / text_embeddings.norm(dim=-1, keepdim=True)\n",
    "            embeddings.append((idxs.item(), normalized_text_embeddings.squeeze(0).cpu().numpy().tolist()))\n",
    "\n",
    "    # Update the JSON file with text embeddings\n",
    "    with open(json_file, 'r+') as f:\n",
    "        data = json.load(f)\n",
    "        for idx, emb in embeddings:\n",
    "            data[idx]['text_embedding'] = emb\n",
    "        f.seek(0)\n",
    "        json.dump(data, f, indent=4)\n",
    "        f.truncate()\n",
    "\n",
    "# Load the model and set up the device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "model.eval()\n",
    "\n",
    "# Specify the path to the JSON file\n",
    "json_file = '/workspaces/finetune/AFINAL/clip/clip_gpt_ootb/clip_new_caption_embedding.json'\n",
    "\n",
    "# Compute text embeddings and update the JSON file\n",
    "compute_text_embeddings_and_update_json(json_file, model, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "model.eval()\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, json_file):\n",
    "        with open(json_file, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        image_path = item['image_path']\n",
    "        # Ensure the image is correctly preprocessed into a 4D tensor\n",
    "        image = preprocess(Image.open(image_path).convert('RGB')).to(device)\n",
    "        return image, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Load and index text embeddings\n",
    "def load_text_embeddings(json_file):\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    path_to_embedding = {}\n",
    "    for item in data:\n",
    "        for path in item['image_paths']:\n",
    "            path_to_embedding[path] = item['text_embedding']\n",
    "    return path_to_embedding\n",
    "\n",
    "# Then in your loop processing each image, ensure you handle the dimensions correctly:\n",
    "def compute_embeddings_and_update_json(dataset_json, text_embeddings_json, output_json):\n",
    "    text_embeddings = load_text_embeddings(text_embeddings_json)\n",
    "    dataset = ImageDataset(dataset_json)\n",
    "    loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "    with open(dataset_json, 'r+') as f:\n",
    "        data = json.load(f)\n",
    "        with torch.no_grad():\n",
    "            for images, idxs in loader:\n",
    "                idx = idxs.item()\n",
    "                image_embeddings = model.encode_image(images).cpu()\n",
    "\n",
    "                # Make sure image_embeddings are squeezed if they are 2D [1, embedding_size]\n",
    "                if image_embeddings.dim() == 2:\n",
    "                    image_embeddings = image_embeddings.squeeze(0)  # Now [embedding_size]\n",
    "\n",
    "                image_path = data[idx]['image_path']\n",
    "                \n",
    "                # Find the corresponding text embedding and ensure it's also squeezed\n",
    "                if image_path in text_embeddings:\n",
    "                    text_embedding = torch.tensor(text_embeddings[image_path], dtype=torch.float32).to(device)\n",
    "                    if text_embedding.dim() == 2:\n",
    "                        text_embedding = text_embedding.squeeze(0)  # Now [embedding_size]\n",
    "\n",
    "                    # Concatenate embeddings ensuring both are 1D [embedding_size]\n",
    "                    combined_embedding = torch.cat((image_embeddings, text_embedding), dim=0).tolist()\n",
    "                    data[idx]['combined_embedding'] = combined_embedding\n",
    "        \n",
    "        f.seek(0)\n",
    "        json.dump(data, f, indent=4)\n",
    "        f.truncate()\n",
    "\n",
    "\n",
    "\n",
    "# Paths to the JSON files\n",
    "dataset_files = [\n",
    "    '/workspaces/finetune/AFINAL/clip/clip_gpt_ootb/output/test_data.json',\n",
    "    '/workspaces/finetune/AFINAL/clip/clip_gpt_ootb/output/train_data.json',\n",
    "    '/workspaces/finetune/AFINAL/clip/clip_gpt_ootb/output/val_data.json'\n",
    "]\n",
    "text_embeddings_json = '/workspaces/finetune/AFINAL/clip/clip_gpt_ootb/clip_new_caption_embedding.json'\n",
    "\n",
    "# Process each dataset file\n",
    "for file in dataset_files:\n",
    "    compute_embeddings_and_update_json(file, text_embeddings_json, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Top-1 Accuracy: 0.4160\n",
      "Average Top-2 Accuracy: 0.5440\n",
      "Average Top-3 Accuracy: 0.5600\n",
      "Average Top-4 Accuracy: 0.6000\n",
      "Average Top-5 Accuracy: 0.6080\n",
      "Average Top-6 Accuracy: 0.6320\n",
      "Average Top-7 Accuracy: 0.6640\n",
      "Average Top-8 Accuracy: 0.6640\n",
      "Average Top-9 Accuracy: 0.6640\n",
      "Average Top-10 Accuracy: 0.6640\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Function to load data from JSON file\n",
    "def load_data(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# Function to find embeddings for images based on image_path matching\n",
    "def find_embeddings(data_set, embeddings_data):\n",
    "    embeddings_list = []\n",
    "    for item in data_set:\n",
    "        for data in embeddings_data:\n",
    "            if data['image_path'] == item['image_path']:\n",
    "                item['combined_embedding'] = data['combined_embedding']\n",
    "                embeddings_list.append(item)\n",
    "                break\n",
    "    return embeddings_list\n",
    "\n",
    "# Function to compute top K accuracy\n",
    "def compute_top_k_accuracy(query_images, gallery_data, k):\n",
    "    total_queries = len(query_images)\n",
    "    top_k_hits = 0\n",
    "\n",
    "    # Convert gallery embeddings and classes to numpy arrays for faster processing\n",
    "    gallery_embeddings = np.array([item['combined_embedding'] for item in gallery_data])\n",
    "    gallery_classes = [item['class'] for item in gallery_data]\n",
    "\n",
    "    for query in query_images:\n",
    "        query_embedding = np.array([query['combined_embedding']])\n",
    "        query_class = query['class']\n",
    "        similarities = cosine_similarity(query_embedding, gallery_embeddings)[0]\n",
    "        top_k_indices = np.argsort(similarities)[-k:]\n",
    "        top_k_classes = np.array(gallery_classes)[top_k_indices]\n",
    "\n",
    "        # Check if the query class is in the top k classes\n",
    "        if query_class in top_k_classes:\n",
    "            top_k_hits += 1\n",
    "\n",
    "    return top_k_hits / total_queries if total_queries > 0 else 0\n",
    "\n",
    "# Load the datasets\n",
    "query_set = load_data('/workspaces/finetune/AFINAL/clip/G&Q BASE/query_set.json')\n",
    "gallery_set = load_data('/workspaces/finetune/AFINAL/clip/G&Q BASE/gallery_set.json')\n",
    "embeddings_data_test = load_data('/workspaces/finetune/AFINAL/clip/clip_gpt_ootb/output/test_data.json')\n",
    "embeddings_data_train = load_data('/workspaces/finetune/AFINAL/clip/clip_gpt_ootb/output/train_data.json')\n",
    "embeddings_data_val = load_data('/workspaces/finetune/AFINAL/clip/clip_gpt_ootb/output/val_data.json')\n",
    "\n",
    "\n",
    "# Find embeddings for each image in the query and gallery sets\n",
    "query_images_with_embeddings_train = find_embeddings(query_set, embeddings_data_train)\n",
    "query_images_with_embeddings_test = find_embeddings(query_set, embeddings_data_test)\n",
    "query_images_with_embeddings_val = find_embeddings(query_set, embeddings_data_val)\n",
    "\n",
    "gallery_images_with_embeddings_train = find_embeddings(gallery_set, embeddings_data_train)\n",
    "gallery_images_with_embeddings_test = find_embeddings(gallery_set, embeddings_data_test)\n",
    "gallery_images_with_embeddings_val = find_embeddings(gallery_set, embeddings_data_val)\n",
    "# Combine the embeddings for query and gallery sets\n",
    "query_images_with_embeddings = query_images_with_embeddings_train + query_images_with_embeddings_test + query_images_with_embeddings_val\n",
    "gallery_images_with_embeddings = gallery_images_with_embeddings_train + gallery_images_with_embeddings_test + gallery_images_with_embeddings_val\n",
    "\n",
    "\n",
    "# Compute the Top-K accuracy\n",
    "for k in range(1, 11):\n",
    "    top_k_accuracy = compute_top_k_accuracy(query_images_with_embeddings, gallery_images_with_embeddings, k)\n",
    "    print(f\"Average Top-{k} Accuracy: {top_k_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 839\n"
     ]
    }
   ],
   "source": [
    "# Input text\n",
    "text = \"\"\"Absolutely, here is a revised product description: This product showcases an individual holding a meticulously crafted piece of a car. The item is characterized by its well-defined edges that are cleanly cut, indicating a high level of precision during its creation. Each corner is sharp and distinct, pointing to a great level of detail and thought put into its design. The color of the car part is not just a simple monotone. Instead, it possesses a vibrant, metallic hue that catches the light and reflects it back in a myriad of glimmering specks. This car piece, held in the person's hand, not only signifies the quality of the product but also adds a touch of realism and relatability. The combination of its sharp edges, defined corners, and radiant color makes this product a noteworthy piece for any car enthusiasts or collectors.\"\"\"\n",
    "\n",
    "# Count the number of characters\n",
    "num_characters = len(text)\n",
    "print(f\"Number of characters: {num_characters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetune-f9B3-yHi-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
