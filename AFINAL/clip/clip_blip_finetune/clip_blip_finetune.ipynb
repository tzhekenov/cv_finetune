{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import clip\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Function to load JSON data\n",
    "def load_json(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# Function to save JSON data\n",
    "def save_json(data, filepath):\n",
    "    with open(filepath, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "# Function to match captions and update data\n",
    "def match_captions(data, caption_data):\n",
    "    for item in data:\n",
    "        for caption_item in caption_data:\n",
    "            if item['image_path'] == caption_item['image_path']:\n",
    "                item['caption'] = caption_item['updated_caption']\n",
    "                break\n",
    "    return data\n",
    "\n",
    "# Load datasets\n",
    "train_data = load_json('/workspaces/finetune/AFINAL/clip/clip_blip_finetune/output/train_data.json')\n",
    "val_data = load_json('/workspaces/finetune/AFINAL/clip/clip_blip_finetune/output/val_data.json')\n",
    "caption_data = load_json('/workspaces/finetune/AFINAL/clip/clip_gpt_ootb/clip_new_caption_embedding.json')\n",
    "\n",
    "# Update captions in the datasets\n",
    "train_data = match_captions(train_data, caption_data)\n",
    "val_data = match_captions(val_data, caption_data)\n",
    "\n",
    "# Save updated datasets\n",
    "save_json(train_data, '/workspaces/finetune/AFINAL/clip/clip_blip_finetune/output/train_data.json')\n",
    "save_json(val_data, '/workspaces/finetune/AFINAL/clip/clip_blip_finetune/output/val_data.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import clip\n",
    "\n",
    "# Load JSON data\n",
    "def load_json(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "train_data = load_json('/workspaces/finetune/AFINAL/clip/clip_blip_finetune/output/train_data.json')\n",
    "val_data = load_json('/workspaces/finetune/AFINAL/clip/clip_blip_finetune/output/val_data.json')\n",
    "caption_data = load_json('/workspaces/finetune/AFINAL/clip/clip_blip_finetune/clip_caption_embedding.json')\n",
    "\n",
    "# Match image paths with captions\n",
    "def match_captions(data, caption_data):\n",
    "    for item in data:\n",
    "        for caption_item in caption_data:\n",
    "            if item['image_path'] == caption_item['image_path']:\n",
    "                item['caption'] = caption_item['caption']\n",
    "                break\n",
    "    return data\n",
    "\n",
    "train_data = match_captions(train_data, caption_data)\n",
    "val_data = match_captions(val_data, caption_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import clip\n",
    "\n",
    "# Load JSON data\n",
    "def load_json(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "train_data = load_json('/workspaces/finetune/AFINAL/clip/clip_blip_finetune/output/train_data.json')\n",
    "val_data = load_json('/workspaces/finetune/AFINAL/clip/clip_blip_finetune/output/val_data.json')\n",
    "caption_data = load_json('/workspaces/finetune/AFINAL/clip/clip_blip_finetune/clip_caption_embedding.json')\n",
    "\n",
    "# Match image paths with captions\n",
    "def match_captions(data, caption_data):\n",
    "    for item in data:\n",
    "        for caption_item in caption_data:\n",
    "            if item['image_path'] == caption_item['image_path']:\n",
    "                item['caption'] = caption_item['caption']\n",
    "                break\n",
    "    return data\n",
    "\n",
    "train_data = match_captions(train_data, caption_data)\n",
    "val_data = match_captions(val_data, caption_data)\n",
    "\n",
    "class CustomClipDataset(Dataset):\n",
    "    def __init__(self, data, preprocess):\n",
    "        self.data = data\n",
    "        self.preprocess = preprocess\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        image = Image.open(item['image_path']).convert('RGB')\n",
    "        image = self.preprocess(image)\n",
    "        caption = item['caption']\n",
    "        label = item['class']\n",
    "        return image, caption, label\n",
    "\n",
    "# Initialize the dataset and dataloaders\n",
    "preprocess = clip.load(\"ViT-B/32\")[1]\n",
    "\n",
    "train_dataset = CustomClipDataset(train_data, preprocess)\n",
    "val_dataset = CustomClipDataset(val_data, preprocess)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import clip\n",
    "\n",
    "class CustomClipModel(nn.Module):\n",
    "    def __init__(self, clip_model):\n",
    "        super(CustomClipModel, self).__init__()\n",
    "        self.clip_model = clip_model\n",
    "\n",
    "        # Freeze all parameters in the CLIP model\n",
    "        for param in self.clip_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Define the new head for combining image and text embeddings\n",
    "        self.image_embedding_dim = self.clip_model.visual.output_dim\n",
    "        self.text_embedding_dim = self.clip_model.text_projection.shape[1]\n",
    "        self.combined_dim = self.image_embedding_dim + self.text_embedding_dim\n",
    "\n",
    "        self.fc = nn.Linear(self.combined_dim, 512)\n",
    "\n",
    "    def forward(self, image, text):\n",
    "        image_features = self.clip_model.encode_image(image)\n",
    "        text_features = self.clip_model.encode_text(text)\n",
    "        combined_features = torch.cat((image_features, text_features), dim=1)\n",
    "        output = self.fc(combined_features)\n",
    "        return output\n",
    "\n",
    "# Load the CLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "model = CustomClipModel(clip_model).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import clip\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load JSON data\n",
    "def load_json(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "train_data = load_json('/workspaces/finetune/AFINAL/clip/clip_blip_finetune/output/train_data.json')\n",
    "val_data = load_json('/workspaces/finetune/AFINAL/clip/clip_blip_finetune/output/val_data.json')\n",
    "caption_data = load_json('/workspaces/finetune/AFINAL/clip/clip_blip_finetune/clip_caption_embedding.json')\n",
    "\n",
    "# Match image paths with captions\n",
    "def match_captions(data, caption_data):\n",
    "    for item in data:\n",
    "        for caption_item in caption_data:\n",
    "            if item['image_path'] == caption_item['image_path']:\n",
    "                item['caption'] = caption_item['caption']\n",
    "                break\n",
    "    return data\n",
    "\n",
    "train_data = match_captions(train_data, caption_data)\n",
    "val_data = match_captions(val_data, caption_data)\n",
    "\n",
    "# Define a dataset class for handling images and captions\n",
    "class CustomClipDataset(Dataset):\n",
    "    def __init__(self, data, preprocess):\n",
    "        self.data = data\n",
    "        self.preprocess = preprocess\n",
    "        self.label_to_index = self.create_label_to_index()\n",
    "        self.index_to_label = {v: k for k, v in self.label_to_index.items()}\n",
    "        self.class_to_images = self.create_class_to_images()\n",
    "\n",
    "    def create_label_to_index(self):\n",
    "        labels = sorted(set(item['class'] for item in self.data))\n",
    "        return {label: idx for idx, label in enumerate(labels)}\n",
    "\n",
    "    def create_class_to_images(self):\n",
    "        class_to_images = {}\n",
    "        for item in self.data:\n",
    "            cls = item['class']\n",
    "            if cls not in class_to_images:\n",
    "                class_to_images[cls] = []\n",
    "            class_to_images[cls].append(item['image_path'])\n",
    "        return class_to_images\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        image = Image.open(item['image_path']).convert('RGB')\n",
    "        image = self.preprocess(image)\n",
    "        caption = item['caption']\n",
    "        label = self.label_to_index[item['class']]\n",
    "        return image, caption, label\n",
    "\n",
    "# Initialize the dataset and dataloaders\n",
    "preprocess = clip.load(\"ViT-B/32\")[1]\n",
    "\n",
    "train_dataset = CustomClipDataset(train_data, preprocess)\n",
    "val_dataset = CustomClipDataset(val_data, preprocess)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the CustomClipModel\n",
    "class CustomClipModel(nn.Module):\n",
    "    def __init__(self, clip_model):\n",
    "        super(CustomClipModel, self).__init__()\n",
    "        self.clip_model = clip_model\n",
    "\n",
    "        # Freeze all parameters in the CLIP model\n",
    "        for param in self.clip_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Define the new head for combining image and text embeddings\n",
    "        self.image_embedding_dim = self.clip_model.visual.output_dim\n",
    "        self.text_embedding_dim = self.clip_model.text_projection.shape[1]\n",
    "        self.combined_dim = self.image_embedding_dim + self.text_embedding_dim\n",
    "\n",
    "        self.fc = nn.Linear(self.combined_dim, 512)\n",
    "\n",
    "    def forward(self, image, text):\n",
    "        image_features = self.clip_model.encode_image(image)\n",
    "        text_features = self.clip_model.encode_text(text)\n",
    "        combined_features = torch.cat((image_features, text_features), dim=1)\n",
    "        output = self.fc(combined_features)\n",
    "        return output\n",
    "\n",
    "# Load the CLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Initialize the custom model\n",
    "model = CustomClipModel(clip_model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to generate triplets\n",
    "def generate_triplets(embeddings, targets, model, dataset):\n",
    "    device = embeddings.device\n",
    "    batch_size = embeddings.size(0)\n",
    "\n",
    "    anchors = []\n",
    "    positives = []\n",
    "    negatives = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        anchor = embeddings[i]\n",
    "        anchor_label = targets[i].item()\n",
    "        anchor_class = dataset.index_to_label[anchor_label]\n",
    "\n",
    "        # Find positive example (same class as anchor)\n",
    "        positive_image_path = random.choice(dataset.class_to_images[anchor_class])\n",
    "        positive_idx = next(i for i, item in enumerate(dataset.data) if item['image_path'] == positive_image_path)\n",
    "        positive_image, positive_caption, _ = dataset[positive_idx]\n",
    "        positive_image = positive_image.unsqueeze(0).to(device)\n",
    "        positive_embedding = model(positive_image, clip.tokenize([positive_caption]).to(device)).squeeze(0)\n",
    "\n",
    "        # Find negative example (different class than anchor)\n",
    "        negative_label = random.choice([label for label in dataset.class_to_images.keys() if label != anchor_class])\n",
    "        negative_image_path = random.choice(dataset.class_to_images[negative_label])\n",
    "        negative_idx = next(i for i, item in enumerate(dataset.data) if item['image_path'] == negative_image_path)\n",
    "        negative_image, negative_caption, _ = dataset[negative_idx]\n",
    "        negative_image = negative_image.unsqueeze(0).to(device)\n",
    "        negative_embedding = model(negative_image, clip.tokenize([negative_caption]).to(device)).squeeze(0)\n",
    "\n",
    "        # Debugging: Print shapes and verify different embeddings\n",
    "        #print(f\"Anchor: {anchor.shape}, Positive: {positive_embedding.shape}, Negative: {negative_embedding.shape}\")\n",
    "        assert anchor_label != dataset.label_to_index[negative_label], \"Anchor and negative have the same label!\"\n",
    "\n",
    "        anchors.append(anchor)\n",
    "        positives.append(positive_embedding)\n",
    "        negatives.append(negative_embedding)\n",
    "\n",
    "    return torch.stack(anchors), torch.stack(positives), torch.stack(negatives)\n",
    "\n",
    "# Define the triplet loss function\n",
    "def triplet_loss(anchor, positive, negative, margin=1.0):\n",
    "    positive_distance = F.pairwise_distance(anchor, positive)\n",
    "    negative_distance = F.pairwise_distance(anchor, negative)\n",
    "    loss = torch.mean(F.relu(positive_distance - negative_distance + margin))\n",
    "    \n",
    "    # Debugging: Print distances and loss\n",
    "    #print(f\"Positive Distance: {positive_distance.mean().item()}, Negative Distance: {negative_distance.mean().item()}, Loss: {loss.item()}\")\n",
    "    \n",
    "    return loss\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function with tqdm progress bars\n",
    "def train_model(model, train_loader, val_loader, optimizer, num_epochs=10, save_dir='model_weights'):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "        \n",
    "        for batch_idx, (images, captions, labels) in enumerate(train_loader_tqdm):\n",
    "            images = images.to(device)\n",
    "            texts = clip.tokenize(captions).to(device)\n",
    "            labels = torch.tensor(labels, dtype=torch.long).to(device)  # Ensure labels are long tensor\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            embeddings = model(images, texts)\n",
    "\n",
    "            anchors, positives, negatives = generate_triplets(embeddings, labels, model, train_dataset)\n",
    "\n",
    "            loss = triplet_loss(anchors, positives, negatives)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            train_loader_tqdm.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "        val_loader_tqdm = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (images, captions, labels) in enumerate(val_loader_tqdm):\n",
    "                images = images.to(device)\n",
    "                texts = clip.tokenize(captions).to(device)\n",
    "                labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "                embeddings = model(images, texts)\n",
    "\n",
    "                anchors, positives, negatives = generate_triplets(embeddings, labels, model, val_dataset)\n",
    "\n",
    "                loss = triplet_loss(anchors, positives, negatives)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                val_loader_tqdm.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] completed. Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Save model weights\n",
    "        torch.save(model.state_dict(), os.path.join(save_dir, f'epoch_{epoch+1}.pth'))\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Train the model\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=1e-4)  # Only train the new head\n",
    "train_losses, val_losses = train_model(model, train_loader, val_loader, optimizer, num_epochs=10)\n",
    "\n",
    "# Plotting the losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Triplet Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated captions have been added to the second JSON file.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load JSON data\n",
    "def load_json(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# Save JSON data\n",
    "def save_json(data, filepath):\n",
    "    with open(filepath, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "# File paths\n",
    "file_with_updated_captions = '/workspaces/finetune/AFINAL/clip/clip_gpt_ootb/clip_new_caption_embedding.json'\n",
    "file_without_updated_captions = '/workspaces/finetune/AFINAL/clip/clip_blip_finetune/clip_caption_embedding.json'\n",
    "\n",
    "# Load datasets\n",
    "data_with_updated_captions = load_json(file_with_updated_captions)\n",
    "data_without_updated_captions = load_json(file_without_updated_captions)\n",
    "\n",
    "# Create a dictionary for quick lookup of updated captions\n",
    "updated_captions_dict = {item['image_path']: item['updated_caption'] for item in data_with_updated_captions}\n",
    "\n",
    "# Update the records in the second dataset\n",
    "for item in data_without_updated_captions:\n",
    "    image_path = item['image_path']\n",
    "    if image_path in updated_captions_dict:\n",
    "        item['updated_caption'] = updated_captions_dict[image_path]\n",
    "\n",
    "# Save the updated dataset\n",
    "save_json(data_without_updated_captions, file_without_updated_captions)\n",
    "\n",
    "print(\"Updated captions have been added to the second JSON file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Initialize the custom model\n",
    "model = CustomClipModel(clip_model).to(device)\n",
    "\n",
    "# Load the pre-trained weights into the custom model\n",
    "checkpoint_path = '/workspaces/finetune/AFINAL/clip/clip_blip_finetune/model_weights/epoch_10.pth'\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "\n",
    "# Define a dataset class specifically for handling text (captions)\n",
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self, json_file):\n",
    "        with open(json_file, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        caption = self.data[idx]['updated_caption']\n",
    "        return caption, idx\n",
    "\n",
    "# Function to compute text embeddings and update the JSON file\n",
    "def compute_text_embeddings_and_update_json(json_file, model, device):\n",
    "    dataset = CaptionDataset(json_file)\n",
    "    loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    embeddings = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for captions, idxs in loader:\n",
    "            captions = clip.tokenize(captions).to(device)\n",
    "            \n",
    "            # Get text embeddings\n",
    "            text_embeddings = model.clip_model.encode_text(captions)\n",
    "            \n",
    "            # Normalize embeddings\n",
    "            normalized_text_embeddings = text_embeddings / text_embeddings.norm(dim=-1, keepdim=True)\n",
    "            embeddings.append((idxs.item(), normalized_text_embeddings.squeeze(0).cpu().numpy().tolist()))\n",
    "            \n",
    "            # Debugging output\n",
    "            print(f\"Processed caption index: {idxs.item()}, embedding: {normalized_text_embeddings.squeeze(0).cpu().numpy().tolist()}\")\n",
    "\n",
    "    # Update the JSON file with text embeddings\n",
    "    with open(json_file, 'r+') as f:\n",
    "        data = json.load(f)\n",
    "        for idx, emb in embeddings:\n",
    "            data[idx]['text_embedding'] = emb\n",
    "        f.seek(0)\n",
    "        json.dump(data, f, indent=4)\n",
    "        f.truncate()\n",
    "\n",
    "    print(f\"Updated {json_file} with text embeddings.\")\n",
    "\n",
    "# Specify the path to the JSON file\n",
    "json_file = '/workspaces/finetune/AFINAL/clip/clip_blip_finetune/clip_caption_embedding.json'\n",
    "\n",
    "# Compute text embeddings and update the JSON file\n",
    "# compute_text_embeddings_and_update_json(json_file, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, json_file):\n",
    "        with open(json_file, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        image_path = item['image_path']\n",
    "        # Ensure the image is correctly preprocessed into a 4D tensor\n",
    "        image = preprocess(Image.open(image_path).convert('RGB')).to(device)\n",
    "        return image, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load and index text embeddings\n",
    "def load_text_embeddings(json_file):\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    path_to_embedding = {}\n",
    "    for item in data:\n",
    "        for path in item['image_paths']:\n",
    "            path_to_embedding[path] = item['text_embedding']\n",
    "    return path_to_embedding\n",
    "\n",
    "# Compute embeddings and update the JSON file\n",
    "def compute_embeddings_and_update_json(dataset_json, text_embeddings_json, output_json):\n",
    "    text_embeddings = load_text_embeddings(text_embeddings_json)\n",
    "    dataset = ImageDataset(dataset_json)\n",
    "    loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "    with open(dataset_json, 'r+') as f:\n",
    "        data = json.load(f)\n",
    "        with torch.no_grad():\n",
    "            for images, idxs in loader:\n",
    "                idx = idxs.item()\n",
    "                image_embeddings = images.to(device)\n",
    "\n",
    "                image_path = data[idx]['image_path']\n",
    "                \n",
    "                # Find the corresponding text embedding\n",
    "                if image_path in text_embeddings:\n",
    "                    text_embedding = torch.tensor(text_embeddings[image_path], dtype=torch.float32).to(device)\n",
    "                    \n",
    "                    # Ensure the image_embeddings tensor is 4D\n",
    "                    if image_embeddings.dim() == 3:\n",
    "                        image_embeddings = image_embeddings.unsqueeze(0)  # Adding batch dimension\n",
    "\n",
    "                    # Get image embeddings\n",
    "                    image_features = model.clip_model.encode_image(image_embeddings)\n",
    "\n",
    "                    # Concatenate embeddings ensuring both are 1D [embedding_size]\n",
    "                    combined_embedding = torch.cat((image_features.squeeze(0), text_embedding), dim=0).unsqueeze(0)\n",
    "\n",
    "                    # Pass the combined embedding through the last trained layer\n",
    "                    final_embedding = model.fc(combined_embedding).squeeze(0).cpu().tolist()\n",
    "                    data[idx]['combined_embedding'] = final_embedding\n",
    "        \n",
    "        f.seek(0)\n",
    "        json.dump(data, f, indent=4)\n",
    "        f.truncate()\n",
    "        \n",
    "# Paths to the JSON files\n",
    "dataset_files = [\n",
    "    '/workspaces/finetune/AFINAL/clip/clip_blip_finetune/output/test_data.json',\n",
    "    '/workspaces/finetune/AFINAL/clip/clip_blip_finetune/output/train_data.json',\n",
    "    '/workspaces/finetune/AFINAL/clip/clip_blip_finetune/output/val_data.json'\n",
    "]\n",
    "text_embeddings_json = '/workspaces/finetune/AFINAL/clip/clip_blip_finetune/clip_caption_embedding.json'\n",
    "\n",
    "# Process each dataset file\n",
    "for file in dataset_files:\n",
    "    compute_embeddings_and_update_json(file, text_embeddings_json, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Top-1 Accuracy: 0.5280\n",
      "Average Top-2 Accuracy: 0.6480\n",
      "Average Top-3 Accuracy: 0.6720\n",
      "Average Top-4 Accuracy: 0.7120\n",
      "Average Top-5 Accuracy: 0.7520\n",
      "Average Top-6 Accuracy: 0.7840\n",
      "Average Top-7 Accuracy: 0.7840\n",
      "Average Top-8 Accuracy: 0.7920\n",
      "Average Top-9 Accuracy: 0.8000\n",
      "Average Top-10 Accuracy: 0.8080\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Function to load data from JSON file\n",
    "def load_data(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# Function to find embeddings for images based on image_path matching\n",
    "def find_embeddings(data_set, embeddings_data):\n",
    "    embeddings_list = []\n",
    "    for item in data_set:\n",
    "        for data in embeddings_data:\n",
    "            if data['image_path'] == item['image_path']:\n",
    "                item['combined_embedding'] = data['combined_embedding']\n",
    "                embeddings_list.append(item)\n",
    "                break\n",
    "    return embeddings_list\n",
    "\n",
    "# Function to compute top K accuracy\n",
    "def compute_top_k_accuracy(query_images, gallery_data, k):\n",
    "    total_queries = len(query_images)\n",
    "    top_k_hits = 0\n",
    "\n",
    "    # Convert gallery embeddings and classes to numpy arrays for faster processing\n",
    "    gallery_embeddings = np.array([item['combined_embedding'] for item in gallery_data])\n",
    "    gallery_classes = [item['class'] for item in gallery_data]\n",
    "\n",
    "    for query in query_images:\n",
    "        query_embedding = np.array([query['combined_embedding']])\n",
    "        query_class = query['class']\n",
    "        similarities = cosine_similarity(query_embedding, gallery_embeddings)[0]\n",
    "        top_k_indices = np.argsort(similarities)[-k:]\n",
    "        top_k_classes = np.array(gallery_classes)[top_k_indices]\n",
    "\n",
    "        # Check if the query class is in the top k classes\n",
    "        if query_class in top_k_classes:\n",
    "            top_k_hits += 1\n",
    "\n",
    "    return top_k_hits / total_queries if total_queries > 0 else 0\n",
    "\n",
    "# Load the datasets\n",
    "query_set = load_data('/workspaces/finetune/AFINAL/clip/G&Q BASE/query_set.json')\n",
    "gallery_set = load_data('/workspaces/finetune/AFINAL/clip/G&Q BASE/gallery_set.json')\n",
    "embeddings_data_test = load_data('/workspaces/finetune/AFINAL/clip/clip_blip_finetune/output/test_data.json')\n",
    "embeddings_data_train = load_data('/workspaces/finetune/AFINAL/clip/clip_blip_finetune/output/train_data.json')\n",
    "embeddings_data_val = load_data('/workspaces/finetune/AFINAL/clip/clip_blip_finetune/output/val_data.json')\n",
    "\n",
    "\n",
    "# Find embeddings for each image in the query and gallery sets\n",
    "query_images_with_embeddings_train = find_embeddings(query_set, embeddings_data_train)\n",
    "query_images_with_embeddings_test = find_embeddings(query_set, embeddings_data_test)\n",
    "query_images_with_embeddings_val = find_embeddings(query_set, embeddings_data_val)\n",
    "\n",
    "gallery_images_with_embeddings_train = find_embeddings(gallery_set, embeddings_data_train)\n",
    "gallery_images_with_embeddings_test = find_embeddings(gallery_set, embeddings_data_test)\n",
    "gallery_images_with_embeddings_val = find_embeddings(gallery_set, embeddings_data_val)\n",
    "# Combine the embeddings for query and gallery sets\n",
    "query_images_with_embeddings = query_images_with_embeddings_train + query_images_with_embeddings_test + query_images_with_embeddings_val\n",
    "gallery_images_with_embeddings = gallery_images_with_embeddings_train + gallery_images_with_embeddings_test + gallery_images_with_embeddings_val\n",
    "\n",
    "\n",
    "# Compute the Top-K accuracy\n",
    "for k in range(1, 11):\n",
    "    top_k_accuracy = compute_top_k_accuracy(query_images_with_embeddings, gallery_images_with_embeddings, k)\n",
    "    print(f\"Average Top-{k} Accuracy: {top_k_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetune-f9B3-yHi-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
